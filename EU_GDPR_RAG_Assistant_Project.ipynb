{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c9af3a",
   "metadata": {},
   "source": [
    "# EU GDPR RAG Assistant ‚Äî Project Notebook\n",
    "\n",
    "\n",
    "**Artifacts included:**\n",
    "- End-to-end RAG pipeline (PDF - Article chunks - embeddings+FAISS - retrieval - generation)\n",
    "- Evaluation table on sample questions\n",
    "- Full Gradio app code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748633a",
   "metadata": {},
   "source": [
    "## 1. Problem Definition\n",
    "\n",
    "**Goal:** Build a Retrieval-Augmented Generation (RAG) assistant that answers questions about the **EU GDPR** by retrieving relevant **Article** chunks and generating concise, grounded answers with **Article citations**.\n",
    "\n",
    "**Why it matters:** GDPR is long and complex; the assistant enables faster navigation and verifiable answers (citations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb396f3",
   "metadata": {},
   "source": [
    "## 2. Data Source & Understanding\n",
    "\n",
    "**Source:** EU GDPR PDF (Regulation (EU) 2016/679) from EUR-Lex.\n",
    "\n",
    "**Data characteristics & challenges:**\n",
    "- Public legal PDF\n",
    "- PDF text extraction can introduce artifacts (soft hyphens, broken words, extra whitespace)\n",
    "- Article headers can be inconsistently extracted - robust chunking needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cace10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install -q pypdf sentence-transformers faiss-cpu transformers accelerate requests pandas numpy gradio\n",
    "#pip install tf-keras\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pypdf import PdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a5b5bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = 'data'\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "PDF_URL = 'https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679'\n",
    "PDF_PATH = os.path.join(RAW_DIR, 'gdpr_2016_679_oj.pdf')\n",
    "PDF_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4508c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already exists: data/raw/gdpr_2016_679_oj.pdf\n"
     ]
    }
   ],
   "source": [
    "def download_file(url: str, out_path: str, chunk_size: int = 1024 * 1024):\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
    "        print(f'PDF already exists: {out_path}')\n",
    "        return\n",
    "    print('Downloading GDPR PDF...')\n",
    "    r = requests.get(url, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    with open(out_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print('Saved:', out_path)\n",
    "\n",
    "download_file(PDF_URL, PDF_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bea02c4",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Text Cleaning\n",
    "Cleaning removes common PDF extraction artifacts while preserving line breaks to support header detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d783b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return ''\n",
    "    s = s.replace('\\u00ad', '')\n",
    "    s = re.sub(r'-\\s*\\n\\s*', '', s)\n",
    "    s = re.sub(r'\\n{3,}', '\\n\\n', s)\n",
    "    s = re.sub(r'[ \\t]+', ' ', s)\n",
    "    s = re.sub(r'\\b(?:[A-Za-z]\\s){2,}[A-Za-z]\\b', lambda m: m.group(0).replace(' ', ''), s)\n",
    "    for _ in range(4):\n",
    "        s = re.sub(r'(?<!\\w)([A-Za-z]{2,})\\s+([A-Za-z]{1,3})(?!\\w)', r'\\1\\2', s)\n",
    "    s = re.sub(r'\\bAr\\s+ticle\\b', 'Article', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\bChar\\s+ter\\b', 'Charter', s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r'\\s+([.,;:])', r'\\1', s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370e08e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "      <th>source_url</th>\n",
       "      <th>n_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I \\n(Legislative acts) \\nREGULA TIONS \\nREGULA...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>2628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>(4) The processingof personal data shouldbe de...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>5150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>(11) Effe ctive prote ctionof personal data th...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>4883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>household activities could includecor responde...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>5381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>(23) In orderto ensure that natural personsare...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "      <td>4678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page                                               text  \\\n",
       "0     1  I \\n(Legislative acts) \\nREGULA TIONS \\nREGULA...   \n",
       "1     2  (4) The processingof personal data shouldbe de...   \n",
       "2     3  (11) Effe ctive prote ctionof personal data th...   \n",
       "3     4  household activities could includecor responde...   \n",
       "4     5  (23) In orderto ensure that natural personsare...   \n",
       "\n",
       "                                          source_url  n_chars  \n",
       "0  https://eur-lex.europa.eu/legal-content/EN/TXT...     2628  \n",
       "1  https://eur-lex.europa.eu/legal-content/EN/TXT...     5150  \n",
       "2  https://eur-lex.europa.eu/legal-content/EN/TXT...     4883  \n",
       "3  https://eur-lex.europa.eu/legal-content/EN/TXT...     5381  \n",
       "4  https://eur-lex.europa.eu/legal-content/EN/TXT...     4678  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_pages(pdf_path: str) -> pd.DataFrame:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    rows = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        txt = clean_text(page.extract_text() or '')\n",
    "        rows.append({'page': i + 1, 'text': txt, 'source_url': PDF_URL})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['n_chars'] = df['text'].str.len()\n",
    "    return df\n",
    "\n",
    "df_pages = extract_pages(PDF_PATH)\n",
    "df_pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96987fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      88.000000\n",
       "mean     3914.386364\n",
       "std      1009.295241\n",
       "min       285.000000\n",
       "25%      3221.500000\n",
       "50%      3803.000000\n",
       "75%      4869.500000\n",
       "max      5620.000000\n",
       "Name: n_chars, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pages['n_chars'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4057d0a",
   "metadata": {},
   "source": [
    "## 4. System / Model Design\n",
    "\n",
    "**RAG architecture:**\n",
    "1. PDF ‚Üí cleaned text per page\n",
    "2. Article-wise chunking\n",
    "3. Embed each chunk (SentenceTransformers)\n",
    "4. Store embeddings in FAISS index\n",
    "5. Retrieve top-k chunks for a question\n",
    "6. Generate answer using a free LLM (FLAN-T5) constrained to retrieved context\n",
    "\n",
    "**Key design choice:** Article-wise chunks improve interpretability and allow citations as (Article X)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d699dd",
   "metadata": {},
   "source": [
    "## 5. Implementation\n",
    "\n",
    "### 5.1 Article-wise chunking (as used in the app)\n",
    "The chunker detects Article headers, slices text into Article blocks, and deduplicates by keeping the longest chunk per Article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d86d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 77\n",
      "Missing articles: [1, 5, 12, 13, 16, 21, 23, 24, 32, 35, 37, 40, 44, 51, 55, 60, 63, 68, 77, 85, 92, 94]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>article</th>\n",
       "      <th>heading</th>\n",
       "      <th>text</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article_2</td>\n",
       "      <td>2</td>\n",
       "      <td>Article 2</td>\n",
       "      <td>Article 2 \\nMaterial scope \\n1. This Regulatio...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article_3</td>\n",
       "      <td>3</td>\n",
       "      <td>Article 3</td>\n",
       "      <td>Article 3 \\nT err itorial scope \\n1. This Regu...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article_4</td>\n",
       "      <td>4</td>\n",
       "      <td>Article 4</td>\n",
       "      <td>Article 4 \\nDef initionsFor thepur posesof thi...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article_6</td>\n",
       "      <td>6</td>\n",
       "      <td>Article 6</td>\n",
       "      <td>Article 6 \\nLawfulnessof processing \\n1. Proce...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>article_7</td>\n",
       "      <td>7</td>\n",
       "      <td>Article 7</td>\n",
       "      <td>Article 7 \\nConditionsfor consent \\n1. Where p...</td>\n",
       "      <td>https://eur-lex.europa.eu/legal-content/EN/TXT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chunk_id  article    heading  \\\n",
       "1  article_2        2  Article 2   \n",
       "2  article_3        3  Article 3   \n",
       "3  article_4        4  Article 4   \n",
       "4  article_6        6  Article 6   \n",
       "5  article_7        7  Article 7   \n",
       "\n",
       "                                                text  \\\n",
       "1  Article 2 \\nMaterial scope \\n1. This Regulatio...   \n",
       "2  Article 3 \\nT err itorial scope \\n1. This Regu...   \n",
       "3  Article 4 \\nDef initionsFor thepur posesof thi...   \n",
       "4  Article 6 \\nLawfulnessof processing \\n1. Proce...   \n",
       "5  Article 7 \\nConditionsfor consent \\n1. Where p...   \n",
       "\n",
       "                                          source_url  \n",
       "1  https://eur-lex.europa.eu/legal-content/EN/TXT...  \n",
       "2  https://eur-lex.europa.eu/legal-content/EN/TXT...  \n",
       "3  https://eur-lex.europa.eu/legal-content/EN/TXT...  \n",
       "4  https://eur-lex.europa.eu/legal-content/EN/TXT...  \n",
       "5  https://eur-lex.europa.eu/legal-content/EN/TXT...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HDR = re.compile(\n",
    "    r\"(?im)(?:^|\\n)\\s*(A\\s*R\\s*T\\s*I\\s*C\\s*L\\s*E|ARTICLE|Article)\"\n",
    "    r\"\\s*[\\n ]*\\(?\\s*(\\d{1,2})\\s*\\)?\\.?\"\n",
    ")\n",
    "\n",
    "def chunk_by_articles(df_pages: pd.DataFrame, min_chars: int = 200):\n",
    "    full_text = \"\\n\".join(df_pages.sort_values('page')['text'].tolist())\n",
    "    matches = list(HDR.finditer(full_text))\n",
    "    chunks = []\n",
    "    for i, m in enumerate(matches):\n",
    "        art = int(m.group(2))\n",
    "        if not (1 <= art <= 99):\n",
    "            continue\n",
    "        start = m.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(full_text)\n",
    "        block = full_text[start:end].strip()\n",
    "        if len(block) < min_chars:\n",
    "            continue\n",
    "        chunks.append({'article': art, 'heading': f'Article {art}', 'text': block, 'len': len(block)})\n",
    "    df_raw = pd.DataFrame(chunks)\n",
    "    df_best = (\n",
    "        df_raw.sort_values(['article', 'len'], ascending=[True, False])\n",
    "              .drop_duplicates('article', keep='first')\n",
    "              .sort_values('article')\n",
    "    )\n",
    "    df_chunks = pd.DataFrame({\n",
    "        'chunk_id': df_best['article'].apply(lambda x: f'article_{int(x)}'),\n",
    "        'article': df_best['article'].astype(int),\n",
    "        'heading': df_best['heading'],\n",
    "        'text': df_best['text'],\n",
    "        'source_url': PDF_URL,\n",
    "    })\n",
    "    missing = sorted(set(range(1, 100)) - set(df_chunks['article']))\n",
    "    return df_chunks, missing\n",
    "\n",
    "df_chunks, missing = chunk_by_articles(df_pages)\n",
    "print('Chunks:', len(df_chunks))\n",
    "print('Missing articles:', missing)\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c42bd1",
   "metadata": {},
   "source": [
    "### 5.2 Embeddings + FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc1a4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149cec2591354964842accd7f48f88cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index size: 77 dim: 384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "EMBED_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "emb = embedder.encode(df_chunks['text'].tolist(), normalize_embeddings=True, show_progress_bar=True)\n",
    "emb = np.asarray(emb, dtype=np.float32)\n",
    "\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "print('Index size:', index.ntotal, 'dim:', emb.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefb36ad",
   "metadata": {},
   "source": [
    "### 5.3 Retrieval (top-k Articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b209f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6129947900772095,\n",
       "  'chunk_id': 'article_7',\n",
       "  'article': 7,\n",
       "  'heading': 'Article 7',\n",
       "  'text': \"Article 7 \\nConditionsfor consent \\n1. Where processingis basedon consent, the controller shallbe ableto demonstrate thatthe data subjecthas \\nconsentedto processingof hisorher personal data. \\n2. Ifthe data subject's consentis giveninthe contextofawr itten declaration which also concerns other matters, the \\nrequestfor consent shallbe presentedina manner whichis clearly distinguishable fromthe other matters, inan \\nintelli gibleand easily accessibleform, using clearand plain language. Anypartof sucha declaration which constitutesan infr ingementof this Regulation shallnotbe binding. \\n3. The data subject shallhave ther ightto withdrawhis orher consentatany time. The withdrawalof consent shallnot affect thela wfulnessof processing basedon consentbef oreits withdrawal. Priorto giving consent, the data subject \\nshallbe informed thereof. It shallbeas easyto withdraw asto give consent. \\n4. When assessing whether consentis freely given, utmost account shallbe takenof whether, inter alia, the \\nperformanceofa contract, includingthepro visionofaser vice, is conditionalon consenttothe processingof personal \\ndata thatisnot necessaryf orthe performanceof that contract.\",\n",
       "  'source_url': 'https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679'},\n",
       " {'score': 0.5489896535873413,\n",
       "  'chunk_id': 'article_3',\n",
       "  'article': 3,\n",
       "  'heading': 'Article 3',\n",
       "  'text': 'Article 3 \\nT err itorial scope \\n1. This Regulation appliestothe processingof personal datainthe context ofthe activitiesofan establishmentofa \\ncontrollerora processorin theU nion, regardlessof whetherthe processingtakes placein theUni onornot. \\n4.5.2016 L 119/32 Official Journal ofthe European UnionEN\\n2. This Regulation appliestothe processingof personal dataof data subjectswho areinthe Unionbya controlleror \\nprocessornot establishedin theU nion, wherethe processing activitiesare relatedto: \\n(a) the offeringof goodsorser vices, ir respectiveof whetherapa ymentofthe data subjectis required, to such data \\nsubjectsinthe Union; or \\n(b) the monitoringof their behaviourasf aras their behaviour takes place withintheU nion. \\n3. This Regulation appliestothe processingof personal databya controllernot establishedin theU nion, butina \\nplace where Member Statelaw appliesby virtueof public intern ationallaw.',\n",
       "  'source_url': 'https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve(query: str, k: int = 6):\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True).astype(np.float32)\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], idxs[0]):\n",
    "        row = df_chunks.iloc[int(idx)]\n",
    "        results.append({\n",
    "            'score': float(score),\n",
    "            'chunk_id': row['chunk_id'],\n",
    "            'article': int(row['article']),\n",
    "            'heading': row['heading'],\n",
    "            'text': row['text'],\n",
    "            'source_url': row['source_url'],\n",
    "        })\n",
    "    return results\n",
    "\n",
    "retrieve('What conditions must be met for consent to be valid under GDPR?', k=5)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae626e8",
   "metadata": {},
   "source": [
    "### 5.4 Answer generation (Free LLM: FLAN-T5)\n",
    "\n",
    "The generator is prompted to answer only from retrieved context and to output citations as (Article X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9398fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- not later than 72 hoursaf terhaving become aware ofit, notifythe personal data breach tothe super visory authority competentin accordance withAr ticle 55, unlessthe personal data breachis unlikelyto resultinar iskto ther ightsand freedomsof natural persons. (Article 33)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "LLM_NAME = 'google/flan-t5-base'\n",
    "llm = pipeline('text2text-generation', model=LLM_NAME, device=-1)\n",
    "\n",
    "def build_prompt(question: str, contexts: list):\n",
    "    c = contexts[0]\n",
    "    excerpt = re.sub(r'\\s{2,}', ' ', c['text'].replace('\\n', ' ')).strip()[:700]\n",
    "    return (\n",
    "        'You are a GDPR document assistant.\\n'\n",
    "        'Use ONLY the context.\\n'\n",
    "        'Return 3-6 bullet points.\\n'\n",
    "        'Each bullet MUST end with (Article X).\\n\\n'\n",
    "        f'Question: {question}\\n\\n'\n",
    "        f\"Context:\\n(Article {c['article']}) {excerpt}\\n\\n\"\n",
    "        'Answer:\\n'\n",
    "    )\n",
    "\n",
    "def ensure_citations(answer: str, contexts: list) -> str:\n",
    "    art = contexts[0]['article']\n",
    "    cite = f'(Article {art})'\n",
    "    lines = []\n",
    "    for l in answer.splitlines():\n",
    "        l = l.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "        if not re.sub(r'\\(Article\\s+\\d+\\)', '', l).strip():\n",
    "            continue\n",
    "        if not l.startswith('-'):\n",
    "            l = '- ' + l\n",
    "        if '(Article' not in l:\n",
    "            l += ' ' + cite\n",
    "        lines.append(l)\n",
    "    return '\\n'.join(lines).strip()\n",
    "\n",
    "def rag_answer(question: str, k: int = 6):\n",
    "    ctx = retrieve(question, k=k)\n",
    "    prompt = build_prompt(question, ctx)\n",
    "    raw = llm(prompt, max_new_tokens=180, do_sample=False)[0]['generated_text']\n",
    "    ans = ensure_citations(raw, ctx)\n",
    "    return {'question': question, 'answer': ans, 'contexts': ctx}\n",
    "\n",
    "demo = rag_answer('When must a personal data breach be notified to the supervisory authority?', k=6)\n",
    "print(demo['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6d3c9",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "We evaluate qualitatively on a small question set:\n",
    "- verify the retrieved top Article\n",
    "- inspect answer previews and citation format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a35f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>top_article</th>\n",
       "      <th>top_score</th>\n",
       "      <th>answer_preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the lawful bases for processing perso...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.562</td>\n",
       "      <td>- a) inthe courseofan activity whichf alls out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What conditions must be met for consent to be ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.613</td>\n",
       "      <td>- 1. Where processingis basedon consent, the c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What information must be provided under the ri...</td>\n",
       "      <td>90</td>\n",
       "      <td>0.513</td>\n",
       "      <td>- personal data whichthe controlleror processo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does GDPR require regarding security of p...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.552</td>\n",
       "      <td>- a) the offering of goods orser vices, ir res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When must a personal data breach be notified t...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.620</td>\n",
       "      <td>- not later than 72 hoursaf terhaving become a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  top_article  top_score  \\\n",
       "0  What are the lawful bases for processing perso...            2      0.562   \n",
       "1  What conditions must be met for consent to be ...            7      0.613   \n",
       "2  What information must be provided under the ri...           90      0.513   \n",
       "3  What does GDPR require regarding security of p...            3      0.552   \n",
       "4  When must a personal data breach be notified t...           33      0.620   \n",
       "\n",
       "                                      answer_preview  \n",
       "0  - a) inthe courseofan activity whichf alls out...  \n",
       "1  - 1. Where processingis basedon consent, the c...  \n",
       "2  - personal data whichthe controlleror processo...  \n",
       "3  - a) the offering of goods orser vices, ir res...  \n",
       "4  - not later than 72 hoursaf terhaving become a...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_QUESTIONS = [\n",
    "    'What are the lawful bases for processing personal data?',\n",
    "    'What conditions must be met for consent to be valid under GDPR?',\n",
    "    'What information must be provided under the right of access?',\n",
    "    'What does GDPR require regarding security of processing?',\n",
    "    'When must a personal data breach be notified to the supervisory authority?',\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for q in EVAL_QUESTIONS:\n",
    "    out = rag_answer(q, k=6)\n",
    "    top = out['contexts'][0]\n",
    "    rows.append({\n",
    "        'question': q,\n",
    "        'top_article': top['article'],\n",
    "        'top_score': round(top['score'], 3),\n",
    "        'answer_preview': (out['answer'][:240] + '...') if len(out['answer']) > 240 else out['answer'],\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c060d19",
   "metadata": {},
   "source": [
    "## 7. Ethical Considerations\n",
    "\n",
    "- **Not legal advice:** This assistant summarizes GDPR text.\n",
    "- **Grounding & transparency:** Answers are constrained to retrieved context and include Article citations.\n",
    "- **Privacy:** Uses a public document and does not process personal user data.\n",
    "- **Failure modes:** PDF extraction noise and LLM hallucination risk are mitigated by strict prompting and citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eabab67",
   "metadata": {},
   "source": [
    "## 8. Conclusion & Future Work\n",
    "\n",
    "**Conclusion:** Implemented an end-to-end RAG pipeline for GDPR with Article-wise retrieval and free-model generation.\n",
    "\n",
    "**Future work:**\n",
    "- Improve Article segmentation with structure-aware parsing\n",
    "- Add reranking (cross-encoder)\n",
    "- Use a larger-context open-source instruction model\n",
    "- Add automated evaluation with a curated QA set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b81c1",
   "metadata": {},
   "source": [
    "## Appendix A ‚Äî Gradio App Code\n",
    "\n",
    "The following cell contains the complete Gradio app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe45770-1fc0-4a6a-ac4d-e5142555b239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    }
   ],
   "source": [
    "# gradio_app.py\n",
    "# EU GDPR RAG Assistant\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "PDF_URL = \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX%3A32016R0679\"\n",
    "DATA_DIR = \"data\"\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "DEFAULT_PDF_PATH = os.path.join(RAW_DIR, \"gdpr_2016_679_oj.pdf\")\n",
    "GDPR_SOURCE_URL = PDF_URL\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "FREE_LLM_NAME = \"google/flan-t5-base\"  # can change to flan-t5-small for faster CPU\n",
    "\n",
    "\n",
    "# -------------------- IO: DOWNLOAD PDF --------------------\n",
    "def download_file(url: str, out_path: str, chunk_size: int = 1024 * 1024):\n",
    "    if os.path.exists(out_path) and os.path.getsize(out_path) > 0:\n",
    "        return out_path\n",
    "    r = requests.get(url, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# -------------------- TEXT CLEANING --------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    s = s.replace(\"\\u00ad\", \"\")                 # soft hyphen\n",
    "    s = re.sub(r\"-\\s*\\n\\s*\", \"\", s)             # join hyphenated line breaks\n",
    "\n",
    "    # keep line breaks (helpful for header detection)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "\n",
    "    # Join spaced letters: A r t i c l e -> Article\n",
    "    s = re.sub(r\"\\b(?:[A-Za-z]\\s){2,}[A-Za-z]\\b\",\n",
    "               lambda m: m.group(0).replace(\" \", \"\"), s)\n",
    "\n",
    "    # Join small mid-word splits: Ar ticle -> Article (repeat)\n",
    "    for _ in range(4):\n",
    "        s = re.sub(r\"(?<!\\w)([A-Za-z]{2,})\\s+([A-Za-z]{1,3})(?!\\w)\", r\"\\1\\2\", s)\n",
    "\n",
    "    s = re.sub(r\"\\bAr\\s+ticle\\b\", \"Article\", s, flags=re.IGNORECASE)\n",
    "    s = re.sub(r\"\\bChar\\s+ter\\b\", \"Charter\", s, flags=re.IGNORECASE)\n",
    "\n",
    "    # tidy spacing before punctuation\n",
    "    s = re.sub(r\"\\s+([.,;:])\", r\"\\1\", s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# -------------------- PDF -> PAGES --------------------\n",
    "def extract_pages(pdf_path: str) -> pd.DataFrame:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    rows = []\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        txt = clean_text(page.extract_text() or \"\")\n",
    "        rows.append({\"page\": i + 1, \"text\": txt, \"source_url\": GDPR_SOURCE_URL})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"n_chars\"] = df[\"text\"].str.len()\n",
    "    return df\n",
    "\n",
    "\n",
    "# -------------------- ARTICLE CHUNKING (ROBUST LINE-BASED) --------------------\n",
    "ARTICLE_WORD = re.compile(\n",
    "    r\"(?i)^(?:A\\s*R\\s*T\\s*I\\s*C\\s*L\\s*E|ARTICLE|Article)\\b\\s*(\\d+)?\\b.*$\"\n",
    ")\n",
    "\n",
    "def parse_article_number(s: str):\n",
    "    s = s.strip()\n",
    "\n",
    "    # allow: \"49\", \"49.\", \"(49)\", \"(49).\"\n",
    "    m = re.match(r\"^\\(?\\s*(\\d{1,3})\\s*\\)?\\.?\\s*$\", s)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "\n",
    "    # allow spaced digits: \"4 9\" or \"9 2\"\n",
    "    m2 = re.match(r\"^(\\d)\\s+(\\d)\\s*$\", s)\n",
    "    if m2:\n",
    "        return int(m2.group(1) + m2.group(2))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def chunk_by_articles(df_pages: pd.DataFrame, min_chars: int = 50, lookahead: int = 10):\n",
    "    full_text = \"\\n\".join(df_pages.sort_values(\"page\")[\"text\"].tolist())\n",
    "    lines = full_text.splitlines()\n",
    "\n",
    "    headers = []  # (line_index, article_number, heading_line)\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        m = ARTICLE_WORD.match(line)\n",
    "        if m:\n",
    "            num = m.group(1)\n",
    "            heading = line\n",
    "\n",
    "            # Look ahead for number if missing\n",
    "            if num is None:\n",
    "                for look in range(1, lookahead + 1):\n",
    "                    if i + look >= len(lines):\n",
    "                        break\n",
    "                    cand = lines[i + look].strip()\n",
    "                    maybe = parse_article_number(cand)\n",
    "                    if maybe is not None and 1 <= maybe <= 99:\n",
    "                        num = str(maybe)\n",
    "                        heading = f\"{heading} {cand}\"\n",
    "                        break\n",
    "\n",
    "            if num is not None:\n",
    "                art_num = int(num)\n",
    "                if 1 <= art_num <= 99:\n",
    "                    headers.append((i, art_num, heading))\n",
    "        i += 1\n",
    "\n",
    "    # Slice blocks between headers\n",
    "    chunks = []\n",
    "    for idx, (line_i, art_num, heading) in enumerate(headers):\n",
    "        start = line_i\n",
    "        end = headers[idx + 1][0] if idx + 1 < len(headers) else len(lines)\n",
    "        block = \"\\n\".join(lines[start:end]).strip()\n",
    "        chunks.append({\"article\": art_num, \"heading\": heading, \"text\": block, \"len\": len(block)})\n",
    "\n",
    "    df_raw = pd.DataFrame(chunks)\n",
    "    if df_raw.empty:\n",
    "        df_chunks = pd.DataFrame(columns=[\"chunk_id\", \"article\", \"heading\", \"text\", \"source_url\"])\n",
    "        return df_chunks, list(range(1, 100))\n",
    "\n",
    "    # Deduplicate: keep the longest chunk per article (removes TOC duplicates / repeated headers)\n",
    "    df_best = (\n",
    "        df_raw.sort_values([\"article\", \"len\"], ascending=[True, False])\n",
    "              .drop_duplicates(\"article\", keep=\"first\")\n",
    "              .sort_values(\"article\")\n",
    "    )\n",
    "\n",
    "    # Drop only tiny junk\n",
    "    df_best = df_best[df_best[\"len\"] >= min_chars]\n",
    "\n",
    "    df_chunks = pd.DataFrame({\n",
    "        \"chunk_id\": df_best[\"article\"].apply(lambda x: f\"article_{int(x)}\"),\n",
    "        \"article\": df_best[\"article\"].astype(int),\n",
    "        \"heading\": df_best[\"heading\"],\n",
    "        \"text\": df_best[\"text\"],\n",
    "        \"source_url\": GDPR_SOURCE_URL,\n",
    "    })\n",
    "\n",
    "    missing = sorted(set(range(1, 100)) - set(df_chunks[\"article\"]))\n",
    "    return df_chunks, missing\n",
    "\n",
    "\n",
    "# -------------------- OPTIONAL: DETERMINISTIC ARTICLE 6 LIST EXTRACTION --------------------\n",
    "def lawful_bases_from_article6(article6_text: str):\n",
    "    \"\"\"\n",
    "    Extract ONLY the first (a)-(f) list in Article 6(1) (the lawful bases).\n",
    "    Stops after (f) to avoid picking up later (a)-(f) lists in other paragraphs.\n",
    "    \"\"\"\n",
    "    t = re.sub(r\"\\s{2,}\", \" \", article6_text.replace(\"\\n\", \" \")).strip()\n",
    "\n",
    "    # Start at \"1.\" if present\n",
    "    m1 = re.search(r\"\\b1\\.\\s\", t)\n",
    "    if m1:\n",
    "        t = t[m1.start():]\n",
    "\n",
    "    bullets = []\n",
    "    for key in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]:\n",
    "        m = re.search(rf\"\\({key}\\)\\s*(.*?)(?=\\([a-f]\\)\\s*|$)\", t, flags=re.IGNORECASE)\n",
    "        if not m:\n",
    "            return None\n",
    "        content = m.group(1).strip().rstrip(\";. \")\n",
    "        bullets.append(f\"- ({key}) {content}. (Article 6)\")\n",
    "        t = t[m.end():]\n",
    "\n",
    "    return \"\\n\".join(bullets)\n",
    "\n",
    "\n",
    "# -------------------- BUILD INDEX (CACHE IN MEMORY) --------------------\n",
    "_CACHE = {}\n",
    "\n",
    "def build_or_get_index(pdf_path: str):\n",
    "    \"\"\"\n",
    "    Builds: df_chunks, faiss_index, embedder, llm, missing\n",
    "    Cached for speed.\n",
    "    \"\"\"\n",
    "    key = os.path.abspath(pdf_path)\n",
    "    if key in _CACHE:\n",
    "        return _CACHE[key]\n",
    "\n",
    "    df_pages = extract_pages(pdf_path)\n",
    "    df_chunks, missing = chunk_by_articles(df_pages, min_chars=50, lookahead=10)\n",
    "\n",
    "    embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "    texts = df_chunks[\"text\"].tolist()\n",
    "    emb = embedder.encode(texts, normalize_embeddings=True, show_progress_bar=False).astype(np.float32)\n",
    "\n",
    "    index = faiss.IndexFlatIP(emb.shape[1])\n",
    "    index.add(emb)\n",
    "\n",
    "    llm = pipeline(\"text2text-generation\", model=FREE_LLM_NAME, device=-1)\n",
    "\n",
    "    _CACHE[key] = (df_chunks, index, embedder, llm, missing)\n",
    "    return _CACHE[key]\n",
    "\n",
    "\n",
    "# -------------------- RETRIEVER --------------------\n",
    "def retrieve(df_chunks: pd.DataFrame, index, embedder, query: str, k: int = 6):\n",
    "    q_emb = embedder.encode([query], normalize_embeddings=True).astype(np.float32)\n",
    "    scores, idxs = index.search(q_emb, k * 8)\n",
    "\n",
    "    q = query.lower()\n",
    "    rescored = []\n",
    "    for base, idx in zip(scores[0], idxs[0]):\n",
    "        row = df_chunks.iloc[int(idx)]\n",
    "        bonus = 0.0\n",
    "        # simple boost for lawful bases questions\n",
    "        if (\"lawful\" in q or \"legal basis\" in q or \"lawful bases\" in q) and int(row[\"article\"]) == 6:\n",
    "            bonus += 0.5\n",
    "        rescored.append((float(base) + bonus, int(idx)))\n",
    "\n",
    "    rescored.sort(reverse=True, key=lambda x: x[0])\n",
    "    top = rescored[:k]\n",
    "\n",
    "    out = []\n",
    "    for score, i in top:\n",
    "        row = df_chunks.iloc[i]\n",
    "        out.append({\n",
    "            \"score\": float(score),\n",
    "            \"chunk_id\": row[\"chunk_id\"],\n",
    "            \"article\": int(row[\"article\"]),\n",
    "            \"heading\": row[\"heading\"],\n",
    "            \"text\": row[\"text\"],\n",
    "            \"source_url\": row[\"source_url\"],\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "# -------------------- GENERATION (FREE LLM) --------------------\n",
    "def build_prompt(question: str, contexts: list, max_ctx: int = 1, per_ctx_chars: int = 700) -> str:\n",
    "    blocks = []\n",
    "    for c in contexts[:max_ctx]:\n",
    "        excerpt = re.sub(r\"\\s{2,}\", \" \", c[\"text\"].replace(\"\\n\", \" \")).strip()[:per_ctx_chars]\n",
    "        blocks.append(f\"(Article {c['article']}) {excerpt}\")\n",
    "\n",
    "    return (\n",
    "        \"You are a GDPR document assistant.\\n\"\n",
    "        \"Use ONLY the provided context excerpts.\\n\"\n",
    "        \"Return 3-6 bullet points.\\n\"\n",
    "        \"Each bullet MUST end with a citation like (Article X).\\n\"\n",
    "        \"If the answer is not in the context, say: I could not find that in the provided GDPR excerpts.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        \"Context:\\n\" + \"\\n\".join(blocks) + \"\\n\\n\"\n",
    "        \"Answer:\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_citations(answer: str, contexts: list) -> str:\n",
    "    top_art = contexts[0][\"article\"] if contexts else None\n",
    "    cite = f\"(Article {top_art})\" if top_art is not None else \"(Article N/A)\"\n",
    "\n",
    "    lines = []\n",
    "    for l in answer.splitlines():\n",
    "        l = l.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "        if not re.sub(r\"\\(Article\\s+\\d+\\)\", \"\", l).strip():\n",
    "            continue\n",
    "        if not l.startswith(\"-\"):\n",
    "            l = \"- \" + l\n",
    "        if \"(Article\" not in l:\n",
    "            l += \" \" + cite\n",
    "        lines.append(l)\n",
    "\n",
    "    out = \"\\n\".join(lines).strip()\n",
    "    return out if out else answer.strip()\n",
    "\n",
    "\n",
    "def truncate_answer(out: str, max_bullets: int = 8, max_chars: int = 1200) -> str:\n",
    "    lines = [l for l in out.splitlines() if l.strip()]\n",
    "    if len(lines) > max_bullets:\n",
    "        out = \"\\n\".join(lines[:max_bullets])\n",
    "    if len(out) > max_chars:\n",
    "        out = out[:max_chars].rstrip() + \"...\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def rag_answer_core(pdf_path: str, question: str, top_k: int = 6, show_passages: bool = True):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return (\n",
    "            f\"‚ùå PDF not found at: `{pdf_path}`\\n\\n\"\n",
    "            f\"Put the file there or download from: {PDF_URL}\",\n",
    "            \"\"\n",
    "        )\n",
    "\n",
    "    df_chunks, index, embedder, llm, missing = build_or_get_index(pdf_path)\n",
    "\n",
    "    hits = retrieve(df_chunks, index, embedder, question, k=top_k)\n",
    "    if not hits:\n",
    "        return \"I could not find that in the provided GDPR excerpts.\", \"\"\n",
    "\n",
    "    q = question.lower()\n",
    "\n",
    "    # Deterministic Article 6 lawful bases extraction (optional)\n",
    "    out = None\n",
    "    if (\"lawful\" in q and (\"basis\" in q or \"bases\" in q or \"legal basis\" in q)) and hits[0][\"article\"] == 6:\n",
    "        out = lawful_bases_from_article6(hits[0][\"text\"])\n",
    "\n",
    "    if not out:\n",
    "        prompt = build_prompt(question, hits, max_ctx=1, per_ctx_chars=700)\n",
    "        raw = llm(prompt, max_new_tokens=180, do_sample=False)[0][\"generated_text\"]\n",
    "        out = ensure_citations(raw, hits)\n",
    "\n",
    "    out = truncate_answer(out, max_bullets=8, max_chars=1200)\n",
    "\n",
    "    # Build \"retrieved passages\" text\n",
    "    passages_md = \"\"\n",
    "    if show_passages:\n",
    "        parts = []\n",
    "        for h in hits[:top_k]:\n",
    "            snippet = re.sub(r\"\\s{2,}\", \" \", h[\"text\"].replace(\"\\n\", \" \")).strip()\n",
    "            snippet = snippet[:900] + (\"...\" if len(snippet) > 900 else \"\")\n",
    "            parts.append(\n",
    "                f\"### Article {h['article']} ‚Äî score {h['score']:.3f}\\n\"\n",
    "                f\"**Heading:** {h['heading']}\\n\\n\"\n",
    "                f\"{snippet}\\n\\n\"\n",
    "                f\"Source: {h['source_url']}\\n\"\n",
    "            )\n",
    "        passages_md = \"\\n\\n---\\n\\n\".join(parts)\n",
    "\n",
    "    # Optional: missing articles note (kept minimal; can remove if you don't want it)\n",
    "    if missing:\n",
    "        out += f\"\\n\\n> Note: Some articles were not detected by PDF extraction/chunking: {missing}\"\n",
    "\n",
    "    return out, passages_md\n",
    "\n",
    "\n",
    "# -------------------- GRADIO UI --------------------\n",
    "SAMPLE_QUESTIONS = [\n",
    "    \"What are the lawful bases for processing personal data?\",          # Article 6\n",
    "    \"What conditions must be met for consent to be valid under GDPR?\",  # Article 7\n",
    "    \"What information must be provided under the right of access?\",     # Article 15\n",
    "    \"When must a personal data breach be notified to the supervisory authority?\",  # Article 33\n",
    "    \"What fines can be imposed under GDPR?\",                            # Article 83\n",
    "]\n",
    "\n",
    "\n",
    "def fill_from_sample(x):\n",
    "    return x or \"\"\n",
    "\n",
    "def ui_answer(pdf_path, question, top_k, show_passages):\n",
    "    ans, passages = rag_answer_core(pdf_path, question, top_k=top_k, show_passages=show_passages)\n",
    "    return ans, passages\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"EU GDPR RAG Assistant\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "# üìò EU GDPR RAG Assistant (Gradio)\n",
    "\n",
    "**Pipeline:** PDF ‚Üí Article-wise chunks ‚Üí embeddings + FAISS ‚Üí retrieve ‚Üí free LLM answer + citations  \n",
    "‚ö†Ô∏è **Informational only** ‚Äî not legal advice.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        pdf_path_in = gr.Textbox(label=\"GDPR PDF path\", value=DEFAULT_PDF_PATH)\n",
    "        top_k_in = gr.Slider(label=\"Top-k retrieved Articles\", minimum=3, maximum=10, value=6, step=1)\n",
    "        show_passages_in = gr.Checkbox(label=\"Show retrieved passages\", value=True)\n",
    "\n",
    "    with gr.Row():\n",
    "        sample_dd = gr.Dropdown(label=\"Sample questions\", choices=[\"\"] + SAMPLE_QUESTIONS, value=\"\")\n",
    "        question_in = gr.Textbox(label=\"Question\", lines=2, placeholder=\"Ask a GDPR question...\")\n",
    "\n",
    "    sample_dd.change(fn=fill_from_sample, inputs=sample_dd, outputs=question_in)\n",
    "\n",
    "    run_btn = gr.Button(\"Answer\")\n",
    "\n",
    "    answer_out = gr.Markdown(label=\"Answer\")\n",
    "    passages_out = gr.Markdown(label=\"Top Retrieved Articles\")\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=ui_answer,\n",
    "        inputs=[pdf_path_in, question_in, top_k_in, show_passages_in],\n",
    "        outputs=[answer_out, passages_out],\n",
    "    )\n",
    "\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "### Run notes\n",
    "- If the PDF doesn't exist, download it from EUR-Lex and save it at the path above.\n",
    "- You can also run this as a script:\n",
    "  - `python gradio_app.py`\n",
    "  - then open the printed local URL.\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# -------------------- ENTRYPOINT --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure PDF is present (download if missing)\n",
    "    try:\n",
    "        if not os.path.exists(DEFAULT_PDF_PATH):\n",
    "            download_file(PDF_URL, DEFAULT_PDF_PATH)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    demo.launch(inline=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d50ebc-644d-46e3-a00b-3bd5f028f0a1",
   "metadata": {},
   "source": [
    "## **Demo Launch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f336b6b4-dc5d-45ad-9c31-f3b62eafe970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(inline=True, prevent_thread_lock=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420aff1e-d936-4e85-95c0-4074f348c841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
